---
layout: post
title:  "scrapy"
date:   2018-07-4 11:09:00
categories: scrapy
tags: scrapy
excerpt: scrapy
mathjax: true
---

#编码

计算机只能处理数字0和1，文本需要转换成数字才能处理，0或1被称为一个bit，
计算机中8个bit作为一个字节。所以通过二进制转换成十进制，一个字节能表示
最大的数字就是255    
计算机是美国人发明的，所以一个字节可以表示所有字符了，所以ASCII
（一个字节）编码就是美国的标准编码   
但是ASCII处理中文明显是不够的，中文不止255个汉字，所以中国制定了
GB2312编码，用两个字节表示一个汉字。GB2312还把ASCII包含进去了，同理，其他
国家为了解决这个问题都发展了一套字节的编码，各自的标准都不同，而且各种标准
也越来越多，如果出现多种语言混合显示就一定会出现乱码。   
unicode的出现解决了这个问题，它可以将所有语言统一到一套编码里   
    
ASCII和unicode编码：    
1、字母A用ASCII编码二进制是0100 0001，十进制是65   
2、汉字“中”超过了ASCII的编码范围，用unicode编码二进制是01001110 00101101，
十进制是20013    
3、A用unicode编码只需要在前面补0，二进制是00000000 0100 0001    
    
乱码问题解决了，但是如果内容全是英文，unicode编码相对于ASCII，存储空间以及传输速度
都大上一倍多，
     
uft-8，把英文变长一个字节，汉字3个字节，生僻字4-6个字节，如果传输大量英文，utf8就有
优势了，相对于unicode可以减少一倍的存储空间，也可以减少传输的数据量。但是在编程中，
utf8就没有unicode在内存中处理的容易，因为utf8长度不一样，但是unicode长度是统一的。   
所以通常我们在读取文件时，需要将utf8转换为unicode，而在我们保存文件的时候，需要将unicode
转换成utf8   
在python2中，默认是acsii编码，要注意编码的转换,下面是在linux中执行的代码：    
```
import sys
sys.getdefaultencoding()
#查看编码
str = "python你好"   
str_u = u"python你好"
str.decode("utf8").encode("utf8")
#先把str通过decode转换成unicode，再通过encode转换成utf8
str_u.encode("utf8")
```
在python3中，都是unicode，不需要在定义字符串前面加"u"

#xpath   
xpath可以通过类似windows中文件路径的方式来定位xml和html中的一个具体的参数（元素）    
xpath包含了标准函数库    
xpath是一个w3c的标准   
article选取所有article元素的所有子节点   
/article选取根元素article    
article/a选取所有属于article的子元素的a元素，注意是子元素，不是后代元素   
//div选取所有div子元素（不论出现在文档的任何地方），相当于取出的是所有后代元素   
article//div选取所有属于article元素的后代的div元素，不管出现在article之下的任何位置    
//@class选取所有名为html中class的属性的值   
/article/div[1]选取属于article子元素的第一个div元素   
/article/div[last()]选取属于article子元素的最后一个div元素    
/article/div[last()-1]选取属于article子元素的倒数第二个div元素    
//div[@lang]选取所有拥有lang属性的div元素      
//div[@lang='eng']选取所有lang属性为eng的div元素     
/div/*属于div的所有子节点   
//*所有元素    
//div[@*]所有带属性的div的元素   
/div/a | //div/p 所有div元素的a和p元素
//span | //ul 所有span和ul元素   
article/div/p | //span 所有属于article下面div元素中的p元素和所有span元素    



#Scrapy   

pycharm建project。
配置git
装环境:   
```
pip install scrapy
```
pycharm并不支持直接创建scrapy的project，所以我们需要手动创建：   
```
scrapy startproject SpiderProject
```

genspider可以直接创建spider的模板，如：   
```
scrapy genspider jobbole blog.jobbole.com
```
可以通过scrapy genspider -h 查看相关帮助   
scrapy genspider -l 查看可用模板    
  
settings.py下还需要设置ROBOTSTXT_OBEY,否则会过滤掉不符合ROBOTS协议的url：   
```
ROBOTSTXT_OBEY = False
```
windows下还需要装个win32模块：   
```
pip install -i https://pypi.douban.com/simple pypiwin32
```

     
```
pip install beautifulsoup4
```   
验证,注意大小写：  
```
from bs4 import BeautifulSoup
```






