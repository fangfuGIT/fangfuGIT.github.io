---
layout: post
title:  "scrapy"
date:   2018-07-4 11:09:00
categories: scrapy
tags: scrapy
excerpt: scrapy
mathjax: true
---


# 编码

计算机只能处理数字0和1，文本需要转换成数字才能处理，0或1被称为一个bit，
计算机中8个bit作为一个字节。所以通过二进制转换成十进制，一个字节能表示
最大的数字就是255    
计算机是美国人发明的，所以一个字节可以表示所有字符了，所以ASCII
（一个字节）编码就是美国的标准编码   
但是ASCII处理中文明显是不够的，中文不止255个汉字，所以中国制定了
GB2312编码，用两个字节表示一个汉字。GB2312还把ASCII包含进去了，同理，其他
国家为了解决这个问题都发展了一套字节的编码，各自的标准都不同，而且各种标准
也越来越多，如果出现多种语言混合显示就一定会出现乱码。   
unicode的出现解决了这个问题，它可以将所有语言统一到一套编码里   
    
ASCII和unicode编码：    
1、字母A用ASCII编码二进制是0100 0001，十进制是65   
2、汉字“中”超过了ASCII的编码范围，用unicode编码二进制是01001110 00101101，
十进制是20013    
3、A用unicode编码只需要在前面补0，二进制是00000000 0100 0001    
    
乱码问题解决了，但是如果内容全是英文，unicode编码相对于ASCII，存储空间以及传输速度
都大上一倍多，
     
uft-8，把英文变长一个字节，汉字3个字节，生僻字4-6个字节，如果传输大量英文，utf8就有
优势了，相对于unicode可以减少一倍的存储空间，也可以减少传输的数据量。但是在编程中，
utf8就没有unicode在内存中处理的容易，因为utf8长度不一样，但是unicode长度是统一的。   
所以通常我们在读取文件时，需要将utf8转换为unicode，而在我们保存文件的时候，需要将unicode
转换成utf8   
在python2中，默认是acsii编码，要注意编码的转换,下面是在linux中执行的代码：    
```

import sys
sys.getdefaultencoding()
#查看编码
str = "python你好"   
str_u = u"python你好"
str.decode("utf8").encode("utf8")
#先把str通过decode转换成unicode，再通过encode转换成utf8
str_u.encode("utf8")
```
在python3中，都是unicode，不需要在定义字符串前面加"u"

# xpath   
xpath可以通过类似windows中文件路径的方式来定位xml和html中的一个具体的参数（元素）    
xpath包含了标准函数库    
xpath是一个w3c的标准   
article选取所有article元素的所有子节点   
/article选取根元素article    
article/a选取所有属于article的子元素的a元素，注意是子元素，不是后代元素   
//div选取所有div子元素（不论出现在文档的任何地方），相当于取出的是所有后代元素   
article//div选取所有属于article元素的后代的div元素，不管出现在article之下的任何位置    
//@class选取所有名为html中class的属性的值   
/article/div[1]选取属于article子元素的第一个div元素   
/article/div[last()]选取属于article子元素的最后一个div元素    
/article/div[last()-1]选取属于article子元素的倒数第二个div元素    
//div[@lang]选取所有拥有lang属性的div元素      
//div[@lang='eng']选取所有lang属性为eng的div元素     
/div/*属于div的所有子节点   
//*所有元素    
//div[@*]所有带属性的div的元素   
/div/a | //div/p 所有div元素的a和p元素
//span | //ul 所有span和ul元素   
article/div/p | //span 所有属于article下面div元素中的p元素和所有span元素    

对于scrapy来说，需要通过xpath或css获取网页上的各种元素。


# Scrapy   

pycharm建project。
配置git
装环境:   
```
pip install scrapy
```
pycharm并不支持直接创建scrapy的project，所以我们需要手动创建：   
```
scrapy startproject SpiderProject
```

genspider可以直接创建spider的模板，如：   
```
scrapy genspider ttsla www.ttsla.com
```
可以通过scrapy genspider -h 查看相关帮助   
scrapy genspider -l 查看可用模板    
  
如果是windows下还需要装个win32模块：   
```
pip install -i https://pypi.douban.com/simple pypiwin32
```

在命令行中调试选择器：   
```
scrapy shell www.ttsla.com
```
这样我们就可以很方便的在命令行中通过response.css调试选择器，并把正确的调试结果copy到代码中。   

实现简单的爬虫功能需要改三个文件：   
```
settings.py
items.py
ttlsa.py
```
settings.py下需要设置ROBOTSTXT_OBEY,否则会过滤掉不符合ROBOTS协议的url：   
```
ROBOTSTXT_OBEY = False
```
settings.py中还需要配置ITEM_PIPELINES，这一段代码默认是禁用的：
```
ITEM_PIPELINES = {
   'ttlsaSpider.pipelines.TtlsaspiderPipeline': 300,
}
```
item.py中定义爬虫爬取结果的结构，可以定义多个，比如标题定义类似于这种：
```
class TtlsaspiderItem(scrapy.Item):
    title = scrapy.Field()
```
ttlsa.py这个是自定义的蜘蛛的名字，名字可能是其他的，我这里只是举例。如果我们之前执行了创建模板的命令，系统会自动生成这个文件。
其中的parse函数需要自己来写。要注意相互调用的关系。比如最简单的：
```
    def parse(self, response):
        item = TtlsaspiderItem()
        nodes = response.css(".entry-header .entry-title a")
        item['标题'] = nodes.css("::text").extract()
        yield item
```
注意在定义start_urls的时候，要配置好完整的url路径：   
```
start_urls = ['http://www.ttlsa.com']
```

配好这三个文件，我们就可以scrapy crawl ttlsa -o result.csv来跑一下，查看csv文件来验证结果。     
    

另外，我们需要了解xpath取数据和css取数据，通过xpath取出数据，如：   
```
title = response.xpath('//*[@class="grid-8"]/div[1]/div[1]/h1/text()').extract()
```
通过css取数据,如：
```
title = response.css(".entry-header .entry-title a::text").extract()
```
response.xpath会返回一个SelectorList对象，extract后会把selector对象转换成list类型    
取出来是一个list，可以通过selector[0]取出值.    
xpath的值可以通过chrome的调试功能copy xpath复制出来,    
下面是复杂一些的，取日期，其实方法是一样的：   
```
date = response.xpath("//*[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip().replace("·", "").strip()
```
strip()方法是去空格和换行符；replace()方法是替换。    
    
contains，我们加上contains，就是class中包含了‘vote-post-up’这个字符串就匹配了，代码如下：   
```
response.xpath("//*[contains(@class, 'vote-post-up')]")
```
通常这种用法适合class包含了多个字符串的时候进行匹配。   

有些时候需要通过正则来匹配，以下代码我们用xpath通过正则过滤掉了中文，只保留数字：    
```
comments_nums = response.xpath("//a[contains(@href, '#article-comment')]/span/text()").extract()[0].strip()
match_re = re.match(".*？(\d+).*", comments_nums)
if match_re:
    comments_nums = match_re.group(1)
```

我们可以通过pipeline保存到json或数据库等，我们可以自定义方法，也可以使用自带的JsonItemExporter方法：
```
#使用scrapy内置JsonItemExporter的方法输出json
class JsonItemExporterPipeline(object):
    def __init__(self):
        self.file = open("result.json", "wb")
        self.export = JsonItemExporter(self.file, ensure_ascii=False, encoding='utf8')
        self.export.start_exporting()

    def close_spider(self):
        self.export.finish_exporting()

    def process_item(self, item, spider):
        self.export.export_item(item)
        return item
```

为了方便调试，我们可以自定义一个main.py,效果和命令行里输入命令是一样的:
```
from scrapy.cmdline import execute

execute(['scrapy', 'crawl', 'ttlsa'])
```
 







     
     
注： 笔记中涉及到的代码在github中.    

     
     
